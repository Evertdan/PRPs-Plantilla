# Analizar Resultados de PRP

## Archivo PRP: $ARGUMENTS

An√°lisis posterior a la ejecuci√≥n de una implementaci√≥n de PRP para capturar lecciones aprendidas, m√©tricas de √©xito y mejoras en la plantilla.

## Proceso de An√°lisis

1.  **Recopilaci√≥n de M√©tricas de Ejecuci√≥n**
    -   Medir el uso de tokens real vs. estimado.
    -   Rastrear el tiempo de implementaci√≥n y las iteraciones.
    -   Documentar los fallos de las pruebas y sus correcciones.
    -   Analizar las m√©tricas de calidad del c√≥digo.

2.  **An√°lisis de Patrones de √âxito**
    -   Identificar qu√© funcion√≥ bien.
    -   Extraer patrones reutilizables.
    -   Documentar elementos de contexto efectivos.
    -   Capturar estrategias de validaci√≥n exitosas.

3.  **Aprendizaje de Patrones de Fallo**
    -   Documentar los problemas encontrados.
    -   Analizar las causas ra√≠z.
    -   Crear estrategias de prevenci√≥n.
    -   Actualizar la base de datos de problemas conocidos (gotchas).

4.  **Recomendaciones de Mejora de la Plantilla**
    -   Identificar lagunas de contexto.
    -   Sugerir mejoras en la validaci√≥n.
    -   Recomendar actualizaciones de la documentaci√≥n.
    -   Proponer nuevos anti-patrones.

5.  **Actualizaciones de la Base de Conocimiento**
    -   A√±adir nuevos patrones de fallo a la base de datos.
    -   Actualizar las m√©tricas de √©xito.
    -   Mejorar la detecci√≥n de funcionalidades similares.
    -   Mejorar la puntuaci√≥n de confianza.

## Marco de An√°lisis

### Recopilaci√≥n de M√©tricas

```bash
# Recopilar m√©tricas de implementaci√≥n
echo "Recopilando m√©tricas de ejecuci√≥n..."

# Obtener estad√≠sticas de git
COMMITS_DURANTE_IMPL=$(git rev-list --count HEAD --since="2 hours ago")
ARCHIVOS_CAMBIADOS=$(git diff --name-only HEAD~$COMMITS_DURANTE_IMPL HEAD | wc -l)
LINEAS_A√ëADIDAS=$(git diff --shortstat HEAD~$COMMITS_DURANTE_IMPL HEAD | grep -o '[0-9]* insertion' | grep -o '[0-9]*' || echo 0)
LINEAS_ELIMINADAS=$(git diff --shortstat HEAD~$COMMITS_DURANTE_IMPL HEAD | grep -o '[0-9]* deletion' | grep -o '[0-9]*' || echo 0)

# Obtener resultados de las pruebas
RESULTADOS_PRUEBAS=$(pytest tests/ --tb=no -q 2>&1 | tail -n 1)
PRUEBAS_PASADAS=$(echo "$RESULTADOS_PRUEBAS" | grep -o '[0-9]* passed' | grep -o '[0-9]*' || echo 0)
PRUEBAS_FALLIDAS=$(echo "$RESULTADOS_PRUEBAS" | grep -o '[0-9]* failed' | grep -o '[0-9]*' || echo 0)

# Obtener m√©tricas de calidad del c√≥digo
PROBLEMAS_RUFF=$(ruff check . 2>&1 | grep -c "error\|warning" || echo 0)
ERRORES_MYPY=$(mypy . 2>&1 | grep -c "error:" || echo 0)

echo "üìä M√©tricas de Implementaci√≥n:"
echo "- Commits: $COMMITS_DURANTE_IMPL"
echo "- Archivos cambiados: $ARCHIVOS_CAMBIADOS"
echo "- L√≠neas a√±adidas: $LINEAS_A√ëADIDAS"
echo "- L√≠neas eliminadas: $LINEAS_ELIMINADAS"
echo "- Pruebas pasadas: $PRUEBAS_PASADAS"
echo "- Pruebas fallidas: $PRUEBAS_FALLIDAS"
echo "- Problemas de Ruff: $PROBLEMAS_RUFF"
echo "- Errores de MyPy: $ERRORES_MYPY"
```

### An√°lisis de la Efectividad del Contexto

```python
# Analizar qu√© elementos del contexto fueron m√°s valiosos
def analizar_efectividad_contexto(archivo_prp):
    """Analiza qu√© partes del PRP fueron m√°s efectivas."""

    # Leer el archivo PRP
    with open(archivo_prp, 'r') as f:
        contenido_prp = f.read()

    # Extraer elementos del contexto
    elementos_contexto = {
        'urls_documentacion': re.findall(r'url: (https?://[^
]+)', contenido_prp),
        'referencias_archivos': re.findall(r'file: ([^
]+)', contenido_prp),
        'problemas_conocidos': re.findall(r'# CRITICAL: ([^
]+)', contenido_prp),
        'patrones': re.findall(r'# PATTERN: ([^
]+)', contenido_prp),
        'ejemplos': re.findall(r'examples/([^
]+)', contenido_prp)
    }

    # Analizar el historial de git para ver qu√© archivos fueron realmente referenciados
    archivos_git = subprocess.check_output(['git', 'log', '--name-only', '--pretty=format:', '--since=2 hours ago']).decode().strip().split('\n')

    # Calcular puntuaciones de efectividad
    puntuaciones_efectividad = {}
    for categoria, elementos in elementos_contexto.items():
        if elementos:
            contador_referenciados = sum(1 for elemento in elementos if any(elemento in archivo_git for archivo_git in archivos_git))
            puntuaciones_efectividad[categoria] = contador_referenciados / len(elementos) * 100
        else:
            puntuaciones_efectividad[categoria] = 0

    return puntuaciones_efectividad
```

### Detecci√≥n de Patrones de Fallo

```python
# Extraer patrones de fallo de la implementaci√≥n
def extraer_patrones_fallo():
    """Extrae nuevos patrones de fallo de la implementaci√≥n."""

    patrones = []

    # Revisar los mensajes de commit de git en busca de indicadores de fallo
    mensajes_commit = subprocess.check_output(['git', 'log', '--oneline', '--since=2 hours ago']).decode().strip().split('\n')

    indicadores_fallo = ['fix', 'error', 'bug', 'issue', 'problem', 'typo', 'mistake']

    for mensaje in mensajes_commit:
        if any(indicador in mensaje.lower() for indicador in indicadores_fallo):
            # Extraer el tipo de fallo
            if 'async' in mensaje.lower():
                patrones.append({
                    'tipo': 'problema_contexto_async',
                    'descripcion': mensaje,
                    'frecuencia': 'alta',
                    'solucion': 'Usar siempre async/await de forma consistente'
                })
            elif 'import' in mensaje.lower():
                patrones.append({
                    'tipo': 'error_importacion',
                    'descripcion': mensaje,
                    'frecuencia': 'media',
                    'solucion': 'Verificar todas las importaciones antes de la implementaci√≥n'
                })
            elif 'type' in mensaje.lower():
                patrones.append({
                    'tipo': 'error_tipo',
                    'descripcion': mensaje,
                    'frecuencia': 'media',
                    'solucion': 'Ejecutar la validaci√≥n de mypy antes de proceder'
                })

    return patrones
```

### Identificaci√≥n de Patrones de √âxito

```python
# Identificar patrones exitosos de la implementaci√≥n
def identificar_patrones_exito():
    """Identifica patrones que llevaron a una implementaci√≥n exitosa."""

    patrones_exito = []

    # Verificar ejecuciones de pruebas limpias
    salida_pruebas = subprocess.check_output(['pytest', 'tests/', '--tb=no', '-q']).decode()
    if 'passed' in salida_pruebas and 'failed' not in salida_pruebas:
        patrones_exito.append({
            'patron': 'pruebas_exhaustivas',
            'descripcion': 'Todas las pruebas pasaron en la implementaci√≥n',
            'recomendacion_reutilizacion': 'Incluir una cobertura de pruebas similar en futuros PRPs'
        })

    # Verificar la calidad del c√≥digo limpio
    salida_ruff = subprocess.check_output(['ruff', 'check', '.', '--quiet']).decode()
    if not salida_ruff.strip():
        patrones_exito.append({
            'patron': 'estilo_codigo_limpio',
            'descripcion': 'No se detectaron problemas de estilo',
            'recomendacion_reutilizacion': 'Mantener patrones de estilo consistentes'
        })

    # Verificar el manejo adecuado de errores
    archivos_python = subprocess.check_output(['find', '.', '-name', '*.py', '-not', '-path', './venv*']).decode().strip().split('\n')

    contador_manejo_errores = 0
    for archivo in archivos_python:
        if archivo.strip():
            with open(archivo, 'r') as f:
                contenido = f.read()
                if 'try:' in contenido and 'except' in contenido:
                    contador_manejo_errores += 1

    if contador_manejo_errores > 0:
        patrones_exito.append({
            'patron': 'manejo_errores_adecuado',
            'descripcion': f'Manejo de errores implementado en {contador_manejo_errores} archivos',
            'recomendacion_reutilizacion': 'Continuar incluyendo patrones de manejo de errores en los PRPs'
        })

    return patrones_exito
```

## Actualizaciones de la Base de Conocimiento

### Base de Datos de Patrones de Fallo

```yaml
# PRPs/knowledge_base/failure_patterns.yaml
patrones_fallo:
  - id: "mezcla_contexto_async"
    descripcion: "Mezclar contextos de c√≥digo s√≠ncrono y as√≠ncrono"
    frecuencia: "alta"
    se√±ales_deteccion:
      - "RuntimeError: cannot be called from a running event loop"
      - "SyncError in async context"
    prevencion:
      - "Usar siempre async/await de forma consistente"
      - "Usar asyncio.run() para llamadas as√≠ncronas de alto nivel"
    bibliotecas_relacionadas: ["asyncio", "aiohttp", "fastapi"]

  - id: "cambios_rompedores_pydantic_v2"
    descripcion: "Cambios de sintaxis de Pydantic v2"
    frecuencia: "media"
    se√±ales_deteccion:
      - "ValidationError: Field required"
      - "AttributeError: 'Field' object has no attribute"
    prevencion:
      - "Usar Field() en lugar de ... para campos opcionales"
      - "Actualizar a la sintaxis v2 para los validadores"
    bibliotecas_relacionadas: ["pydantic", "fastapi"]

  - id: "variable_entorno_faltante"
    descripcion: "Variables de entorno faltantes"
    frecuencia: "media"
    se√±ales_deteccion:
      - "KeyError: 'API_KEY'"
      - "None type has no attribute"
    prevencion:
      - "Revisar siempre la completitud de .env.example"
      - "Usar valores por defecto en la configuraci√≥n"
    bibliotecas_relacionadas: ["python-dotenv", "pydantic-settings"]
```

### Base de Datos de M√©tricas de √âxito

```yaml
# PRPs/knowledge_base/success_metrics.yaml
metricas_exito:
  - tipo_funcionalidad: "integracion_api"
    uso_promedio_tokens: 2500
    tiempo_promedio_implementacion: 35
    tasa_exito: 85
    patrones_comunes:
      - "uso de cliente http as√≠ncrono"
      - "manejo adecuado de errores"
      - "implementaci√≥n de limitaci√≥n de velocidad"

  - tipo_funcionalidad: "operaciones_bd"
    uso_promedio_tokens: 1800
    tiempo_promedio_implementacion: 25
    tasa_exito: 92
    patrones_comunes:
      - "sesiones as√≠ncronas de sqlalchemy"
      - "manejo adecuado de migraciones"
      - "agrupaci√≥n de conexiones"

  - tipo_funcionalidad: "aplicaciones_cli"
    uso_promedio_tokens: 1200
    tiempo_promedio_implementacion: 20
    tasa_exito: 95
    patrones_comunes:
      - "uso de click o typer"
      - "an√°lisis adecuado de argumentos"
      - "salida coloreada"
```

## Generaci√≥n de Informe de An√°lisis

```python
# Generar informe de an√°lisis completo
def generar_informe_analisis(archivo_prp):
    """Genera un informe de an√°lisis completo."""

    informe = {
        'archivo_prp': archivo_prp,
        'timestamp': datetime.now().isoformat(),
        'metricas': recopilar_metricas(),
        'efectividad_contexto': analizar_efectividad_contexto(archivo_prp),
        'patrones_fallo': extraer_patrones_fallo(),
        'patrones_exito': identificar_patrones_exito(),
        'recomendaciones': generar_recomendaciones(),
        'validacion_confianza': validar_puntuacion_confianza(archivo_prp)
    }

    # Guardar en la base de conocimiento
    guardar_en_base_conocimiento(informe)

    # Generar informe legible por humanos
    return formatear_informe_analisis(informe)

def recopilar_metricas():
    """Recopila las m√©tricas de implementaci√≥n."""
    # Estad√≠sticas de Git
    commits = obtener_contador_commits_desde_hace_horas(2)
    archivos_cambiados = obtener_archivos_cambiados_en_commits(commits)
    estadisticas_lineas = obtener_estadisticas_cambio_lineas(commits)

    # Resultados de las pruebas
    resultados_pruebas = ejecutar_suite_pruebas()

    # Calidad del c√≥digo
    metricas_calidad = obtener_metricas_calidad_codigo()

    return {
        'commits': commits,
        'archivos_cambiados': archivos_cambiados,
        'lineas_a√±adidas': estadisticas_lineas['a√±adidas'],
        'lineas_eliminadas': estadisticas_lineas['eliminadas'],
        'pruebas_pasadas': resultados_pruebas['pasadas'],
        'pruebas_fallidas': resultados_pruebas['fallidas'],
        'problemas_ruff': metricas_calidad['problemas_ruff'],
        'errores_mypy': metricas_calidad['errores_mypy'],
        'tiempo_implementacion_minutos': calcular_tiempo_implementacion()
    }

def generar_recomendaciones():
    """Genera recomendaciones para futuros PRPs."""
    recomendaciones = []

    # Analizar la implementaci√≥n actual en busca de oportunidades de mejora
    metricas = recopilar_metricas()

    if metricas['pruebas_fallidas'] > 0:
        recomendaciones.append({
            'tipo': 'pruebas',
            'prioridad': 'alta',
            'sugerencia': 'A√±adir casos de prueba m√°s completos a la plantilla de PRP',
            'justificacion': f"Hubo {metricas['pruebas_fallidas']} fallos en las pruebas durante la implementaci√≥n"
        })

    if metricas['problemas_ruff'] > 5:
        recomendaciones.append({
            'tipo': 'calidad_codigo',
            'prioridad': 'media',
            'sugerencia': 'Incluir una verificaci√≥n de estilo m√°s estricta en el bucle de validaci√≥n',
            'justificacion': f"Se encontraron {metricas['problemas_ruff']} problemas de estilo"
        })

    if metricas['tiempo_implementacion_minutos'] > 60:
        recomendaciones.append({
            'tipo': 'complejidad',
            'prioridad': 'media',
            'sugerencia': 'Desglosar las funcionalidades complejas en PRPs m√°s peque√±os',
            'justificacion': f"La implementaci√≥n tard√≥ {metricas['tiempo_implementacion_minutos']} minutos"
        })

    return recomendaciones

def validar_puntuacion_confianza(archivo_prp):
    """Valida si la puntuaci√≥n de confianza original era precisa."""
    # Extraer la puntuaci√≥n de confianza original del PRP
    with open(archivo_prp, 'r') as f:
        contenido = f.read()

    coincidencia_confianza = re.search(r'Puntuaci√≥n de Confianza: (\d+)/10', contenido)
    confianza_original = int(coincidencia_confianza.group(1)) if coincidencia_confianza else None

    # Calcular indicadores de √©xito reales
    metricas = recopilar_metricas()

    # Puntuaci√≥n basada en resultados reales
    puntuacion_real = 10

    if metricas['pruebas_fallidas'] > 0:
        puntuacion_real -= 2
    if metricas['errores_mypy'] > 0:
        puntuacion_real -= 1
    if metricas['problemas_ruff'] > 10:
        puntuacion_real -= 1
    if metricas['tiempo_implementacion_minutos'] > 90:
        puntuacion_real -= 2
    if metricas['commits'] > 10:  # Demasiadas iteraciones
        puntuacion_real -= 1

    return {
        'confianza_original': confianza_original,
        'puntuacion_real': max(puntuacion_real, 1),
        'precision': abs(confianza_original - puntuacion_real) <= 2 if confianza_original else None
    }
```

## Formato de Salida del Informe

```yaml
üìä Informe de An√°lisis de PRP
======================

üéØ Resumen de la Implementaci√≥n:
- Archivo PRP: {archivo_prp}
- Fecha de Ejecuci√≥n: {timestamp}
- √âxito General: [√âXITO/PARCIAL/FALLIDO]

üìà M√©tricas:
- Commits durante la implementaci√≥n: {commits}
- Archivos cambiados: {archivos_cambiados}
- L√≠neas a√±adidas/eliminadas: {lineas_a√±adidas}/{lineas_eliminadas}
- Tiempo de implementaci√≥n: {tiempo_implementacion_minutos} minutos
- Pruebas: {pruebas_pasadas} pasadas, {pruebas_fallidas} fallidas
- Calidad del c√≥digo: {problemas_ruff} problemas de estilo, {errores_mypy} errores de tipo

üéØ Efectividad del Contexto:
- URLs de documentaci√≥n: {porcentaje_efectividad}% referenciadas
- Referencias de archivos: {porcentaje_efectividad}% usadas
- Ejemplos: {porcentaje_efectividad}% seguidos
- Problemas conocidos: {porcentaje_efectividad}% previnieron problemas

üîç Patrones Descubiertos:
Patrones de √âxito:
{for patron in patrones_exito}
  ‚úÖ {patron.descripcion}
     ‚Üí Reutilizaci√≥n: {patron.recomendacion_reutilizacion}

Patrones de Fallo:
{for patron in patrones_fallo}
  ‚ùå {patron.descripcion}
     ‚Üí Prevenci√≥n: {patron.solucion}

üéØ Validaci√≥n de la Puntuaci√≥n de Confianza:
- Estimaci√≥n original: {confianza_original}/10
- Rendimiento real: {puntuacion_real}/10
- Precisi√≥n de la predicci√≥n: {precision ? "Buena" : "Necesita mejorar"}

üí° Recomendaciones para Futuros PRPs:
{for rec in recomendaciones}
  [{rec.prioridad}] {rec.sugerencia}
  Raz√≥n: {rec.justificacion}

üìö Actualizaciones de la Base de Conocimiento:
- Nuevos patrones de fallo: {contador_nuevos_patrones_fallo}
- M√©tricas de √©xito actualizadas: {contador_metricas_actualizadas}
- Mejoras en la plantilla: {contador_mejoras_plantilla}
```

## Integraci√≥n con la Base de Conocimiento

### Actualizar Base de Datos de Patrones de Fallo

```bash
# Actualizar la base de datos de patrones de fallo
echo "Actualizando la base de datos de patrones de fallo..."

# A√±adir nuevos patrones a PRPs/knowledge_base/failure_patterns.yaml
python3 -c "
import yaml
import sys
from datetime import datetime

# Cargar patrones existentes
try:
    with open('PRPs/knowledge_base/failure_patterns.yaml', 'r') as f:
        db = yaml.safe_load(f) or {'patrones_fallo': []}
except FileNotFoundError:
    db = {'patrones_fallo': []}

# A√±adir nuevos patrones del an√°lisis
nuevos_patrones = extraer_patrones_fallo()
for patron in nuevos_patrones:
    # Comprobar si el patr√≥n ya existe
    existente = next((p for p in db['patrones_fallo'] if p.get('id') == patron['tipo']), None)

    if existente:
        # Actualizar la frecuencia si se vuelve a ver el patr√≥n
        existente['ultima_vez_visto'] = datetime.now().isoformat()
        existente['frecuencia'] = 'alta' if existente.get('frecuencia') == 'media' else existente.get('frecuencia', 'media')
    else:
        # A√±adir nuevo patr√≥n
        db['patrones_fallo'].append({
            'id': patron['tipo'],
            'descripcion': patron['descripcion'],
            'frecuencia': patron['frecuencia'],
            'solucion': patron['solucion'],
            'primera_vez_visto': datetime.now().isoformat(),
            'ultima_vez_visto': datetime.now().isoformat()
        })

# Guardar la base de datos actualizada
with open('PRPs/knowledge_base/failure_patterns.yaml', 'w') as f:
    yaml.dump(db, f, default_flow_style=False, allow_unicode=True)

print(f'Base de datos de patrones de fallo actualizada con {len(nuevos_patrones)} nuevos patrones')
"
```

### Actualizar M√©tricas de √âxito

```bash
# Actualizar las m√©tricas de √©xito para este tipo de funcionalidad
echo "Actualizando las m√©tricas de √©xito..."

python3 -c "
import yaml
from datetime import datetime

# Determinar el tipo de funcionalidad a partir del contenido del PRP
tipo_funcionalidad = determinar_tipo_funcionalidad('$PRP_FILE')
metricas = recopilar_metricas()

# Cargar m√©tricas existentes
try:
    with open('PRPs/knowledge_base/success_metrics.yaml', 'r') as f:
        db = yaml.safe_load(f) or {'metricas_exito': []}
except FileNotFoundError:
    db = {'metricas_exito': []}

# Encontrar o crear una entrada para este tipo de funcionalidad
existente = next((m for m in db['metricas_exito'] if m.get('tipo_funcionalidad') == tipo_funcionalidad), None)

if existente:
    # Actualizar promedios m√≥viles
    existente['implementaciones'] = existente.get('implementaciones', 0) + 1
    existente['uso_promedio_tokens'] = actualizar_promedio_movil(
        existente['uso_promedio_tokens'],
        metricas['tokens_estimados'],
        existente['implementaciones']
    )
    existente['tiempo_promedio_implementacion'] = actualizar_promedio_movil(
        existente['tiempo_promedio_implementacion'],
        metricas['tiempo_implementacion_minutos'],
        existente['implementaciones']
    )
    # Actualizar tasa de √©xito basada en los resultados de las pruebas
    exito = 1 if metricas['pruebas_fallidas'] == 0 else 0
    existente['tasa_exito'] = actualizar_promedio_movil(
        existente['tasa_exito'],
        exito * 100,
        existente['implementaciones']
    )
else:
    # Crear nueva entrada
    tasa_exito = 100 if metricas['pruebas_fallidas'] == 0 else 0
    db['metricas_exito'].append({
        'tipo_funcionalidad': tipo_funcionalidad,
        'implementaciones': 1,
        'uso_promedio_tokens': metricas.get('tokens_estimados', 0),
        'tiempo_promedio_implementacion': metricas['tiempo_implementacion_minutos'],
        'tasa_exito': tasa_exito,
        'ultima_actualizacion': datetime.now().isoformat()
    })

# Guardar m√©tricas actualizadas
with open('PRPs/knowledge_base/success_metrics.yaml', 'w') as f:
    yaml.dump(db, f, default_flow_style=False, allow_unicode=True)
"
```

## Sugerencias de Mejora de Plantilla

```python
# Generar mejoras espec√≠ficas para la plantilla
def sugerir_mejoras_plantilla():
    """Sugiere mejoras espec√≠ficas para las plantillas de PRP."""

    mejoras = []

    # Analizar qu√© contexto faltaba
    contexto_faltante = analizar_contexto_faltante()
    for contexto in contexto_faltante:
        mejoras.append({
            'seccion': 'Contexto',
            'mejora': f'A√±adir validaci√≥n de {contexto["tipo"]} a la plantilla',
            'justificacion': f'La falta de {contexto["descripcion"]} caus√≥ un retraso en la implementaci√≥n'
        })

    # Analizar lagunas de validaci√≥n
    lagunas_validacion = analizar_lagunas_validacion()
    for laguna in lagunas_validacion:
        mejoras.append({
            'seccion': 'Validaci√≥n',
            'mejora': f'A√±adir paso de validaci√≥n de {laguna["tipo"]}',
            'justificacion': f'Habr√≠a detectado {laguna["problema"]} antes'
        })

    # Analizar lagunas de documentaci√≥n
    lagunas_doc = analizar_lagunas_documentacion()
    for laguna in lagunas_doc:
        mejoras.append({
            'seccion': 'Documentaci√≥n',
            'mejora': f'Incluir documentaci√≥n de {laguna["tipo"]}',
            'justificacion': f'Tuve que investigar sobre {laguna["tema"]} durante la implementaci√≥n'
        })

    return mejoras

# Generar autom√°ticamente una plantilla mejorada
def generar_plantilla_mejorada():
    """Genera una plantilla mejorada basada en las lecciones aprendidas."""

    plantilla_base = cargar_plantilla('PRPs/templates/prp_base.md')
    mejoras = sugerir_mejoras_plantilla()

    # Aplicar mejoras a la plantilla
    plantilla_mejorada = aplicar_mejoras(plantilla_base, mejoras)

    # Guardar como plantilla versionada
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    guardar_plantilla(f'PRPs/templates/prp_base_v{timestamp}.md', plantilla_mejorada)

    return plantilla_mejorada
```

## Mecanismo de Auto-actualizaci√≥n

```bash
# Auto-actualizar las plantillas de PRP basadas en el an√°lisis
echo "Comprobando actualizaciones de la plantilla..."

CONTADOR_ANALISIS=$(find PRPs/analysis_reports/ -name "*.yaml" | wc -l)
VERSION_PLANTILLA=$(ls PRPs/templates/prp_base_v*.md 2>/dev/null | tail -n1 | grep -o 'v[0-9_]*' || echo "v1")

# Si tenemos 5+ an√°lisis desde la √∫ltima actualizaci√≥n de la plantilla, generar nueva versi√≥n
if [ "$CONTADOR_ANALISIS" -ge 5 ]; then
    echo "Generando plantilla mejorada basada en an√°lisis recientes..."
    python3 -c "
from analysis_utils import generate_improved_template
improved_template = generate_improved_template()
print('Plantilla mejorada generada con los √∫ltimos aprendizajes')
"
fi
```

## Integraci√≥n con el Comando de Ejecuci√≥n

Actualizar el comando `execute-prp` para ejecutar autom√°ticamente el an√°lisis despu√©s de la finalizaci√≥n:

```bash
# A√±adir al final de execute-prp.md
echo "Ejecutando an√°lisis post-ejecuci√≥n..."
analyze-prp-results "$PRP_FILE"

echo "‚úÖ Implementaci√≥n completa con an√°lisis"
echo "üìä Revisa PRPs/analysis_reports/ para un an√°lisis detallado"
echo "üí° Las mejoras de la plantilla se aplicar√°n a futuros PRPs"
```

## Bucle de Mejora Continua

Este sistema de an√°lisis crea un bucle de mejora continua:

1.  **Ejecutar PRP** ‚Üí Implementar funcionalidad
2.  **Analizar Resultados** ‚Üí Extraer patrones y m√©tricas
3.  **Actualizar Base de Conocimiento** ‚Üí Almacenar aprendizajes
4.  **Mejorar Plantillas** ‚Üí Aplicar aprendizajes a futuros PRPs
5.  **Mejor Contexto** ‚Üí Tasas de √©xito m√°s altas

El sistema aprende de cada implementaci√≥n, haciendo que los futuros PRPs sean m√°s efectivos y reduciendo las tasas de fallo con el tiempo.